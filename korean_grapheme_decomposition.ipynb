{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/home/code/PaddleOCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 682525/682525 [00:10<00:00, 67578.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ppocr.utils.korean_compose import decompose_korean_char\n",
    "import tqdm\n",
    "\n",
    "path = Path(\"/home/datasets/aihub_rec/horizontal_label.txt\")\n",
    "target_path = Path(\"/home/datasets/aihub_rec/horizontal_label2.txt\")\n",
    "target_path.unlink()\n",
    "with open(path) as f:\n",
    "    lines = [line.rstrip(\"\\n\").split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "with open(target_path, \"a+\") as f:\n",
    "    for path, label in tqdm.tqdm(lines):\n",
    "        decomposed = decompose_korean_char(label)\n",
    "        first = \"\".join([x[0] for x in decomposed])\n",
    "        second = \"\".join([x[1] for x in decomposed])\n",
    "        third = \"\".join([x[2] for x in decomposed])\n",
    "        dict_label = {\n",
    "            \"label\":label,\n",
    "            \"first\":first,\n",
    "            \"second\":second,\n",
    "            \"third\":third,\n",
    "        }\n",
    "        f.write(f\"\"\"{path}\\t{str(dict_label).replace(\"'\", '\"')}\\n\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안', '그', '나'), ('녕', '래', '도')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"안녕\"\n",
    "b = \"그래\"\n",
    "c = \"나도\"\n",
    "list(zip(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': '하마하 아아ㄲㅆㅉ', 'second': '아여우 아아ㄲㅆㅉ', 'third': '은응은 읂으ㄲㅆㅉ'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "first_grapheme_dict = {\n",
    "            'ᄀ': '가', 'ᄁ': '까', 'ᄂ': '나', 'ᄃ': '다', 'ᄄ': '따', 'ᄅ': '라',\n",
    "            'ᄆ': '마', 'ᄇ': '바', 'ᄈ': '빠', 'ᄉ': '사', 'ᄊ': '싸', 'ᄋ': '아',\n",
    "            'ᄌ': '자', 'ᄍ': '짜', 'ᄎ': '차', 'ᄏ': '카', 'ᄐ': '타', 'ᄑ': '파', 'ᄒ': '하'\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_first_grapheme(text, type=\"first\"):\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 초성 구하기\n",
    "            choseong_index = (ord(char) - 44032) // 588\n",
    "            # 초성을 문자로 변환하여 결과에 추가\n",
    "            choseong = chr(choseong_index + 0x1100)  # 초성 유니코드 시작값은 0x1100입니다.\n",
    "            result += first_grapheme_dict[choseong]\n",
    "        else: # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "            \n",
    "    return result\n",
    "\n",
    "sceond_grapheme_dict = {\"ᅡ\":\"아\",\"ᅢ\":\"애\",\"ᅣ\":\"야\",\"ᅤ\":\"얘\",\"ᅥ\":\"어\",\"ᅦ\":\"에\",\"ᅧ\":\"여\",\n",
    "\"ᅨ\":\"예\",\"ᅩ\":\"오\",\"ᅪ\":\"와\",\"ᅫ\":\"왜\",\"ᅬ\":\"외\",\"ᅭ\":\"요\",\"ᅮ\":\"우\",\n",
    "\"ᅯ\":\"워\",\"ᅰ\":\"웨\",\"ᅱ\":\"위\",\"ᅲ\":\"유\",\"ᅳ\":\"으\",\"ᅴ\":\"의\",\"ᅵ\":\"이\"}\n",
    "\n",
    "\n",
    "def extract_second_grapheme(text):\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 중성 구하기\n",
    "            jungseong_index = ((ord(char) - 44032) % 588) // 28\n",
    "            # 중성을 문자로 변환하여 결과에 추가\n",
    "            jungseong = chr(jungseong_index + 0x1161)  # 중성 유니코드 시작값은 0x1161입니다.\n",
    "            result += sceond_grapheme_dict[jungseong]\n",
    "        else:\n",
    "            # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "third_grapheme_dict = {\"ᆨ\":\"윽\",\"ᆫ\":\"은\",\"ᆮ\":\"읃\",\"ᆯ\":\"을\",\"ᆷ\":\"음\",\"ᆸ\":\"읍\",\"ᆺ\":\"읏\",\n",
    "\"ᆼ\":\"응\",\"ᆽ\":\"읒\",\"ᆾ\":\"읓\",\"ᆿ\":\"읔\",\"ᇀ\":\"읕\",\"ᇁ\":\"읖\",\"ᇂ\":\"읗\",\n",
    "\"ᆩ\":\"윾\",\"ᆻ\":\"읐\",\"ᆪ\":\"윿\",\"ᆬ\":\"읁\",\"ᆭ\":\"읂\",\"ᆰ\":\"읅\",\"ᆱ\":\"읆\",\n",
    "\"ᆲ\":\"읇\",\"ᆳ\":\"읈\",\"ᆴ\":\"읉\",\"ᆵ\":\"읊\",\"ᆶ\":\"읋\",\"ᆹ\":\"읎\"}\n",
    "\n",
    "\n",
    "def extract_third_grapheme(text):\n",
    "    result = ''\n",
    "    has_jongseong = False  # 받침이 있는지 여부를 판별하기 위한 변수\n",
    "\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 종성 구하기\n",
    "            jongseong_index = (ord(char) - 44032) % 28\n",
    "\n",
    "            # 받침이 있는 경우에만 결과에 추가하고, 받침이 없는 경우 \"으\"를 추가\n",
    "            if jongseong_index != 0:\n",
    "                jongseong = chr(jongseong_index + 0x11A7)  # 종성 유니코드 시작값은 0x11A8이 아닌 0x11A7입니다.\n",
    "                result += third_grapheme_dict[jongseong]\n",
    "            else:\n",
    "                result += '으'\n",
    "        else:\n",
    "            # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "    \n",
    "    return result\n",
    "\n",
    "@functools.lru_cache(maxsize=1000000)\n",
    "def extract_grapheme(text):\n",
    "    return {\n",
    "        \"first\":extract_first_grapheme(text),\n",
    "        \"second\":extract_second_grapheme(text),\n",
    "        \"third\":extract_third_grapheme(text)\n",
    "    }\n",
    "\n",
    "extract_grapheme(\"한명훈 않아ㄲㅆㅉ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ᄒ', 'ᅡ', 'ᆫ'),\n",
       " ('ᄆ', 'ᅧ', 'ᆼ'),\n",
       " ('ᄒ', 'ᅮ', 'ᆫ'),\n",
       " (' ', ' ', ' '),\n",
       " ('ᄋ', 'ᅡ', 'ᆭ'),\n",
       " ('ᄋ', 'ᅡ', '으'),\n",
       " ('ㄲ', 'ㄲ', 'ㄲ'),\n",
       " ('ㅆ', 'ㅆ', 'ㅆ'),\n",
       " ('ㅉ', 'ㅉ', 'ㅉ')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grapheme_to_class = {\n",
    "    \"first\" : {'ᄀ': '가', 'ᄁ': '까', 'ᄂ': '나', 'ᄃ': '다', 'ᄄ': '따', 'ᄅ': '라',\n",
    "                'ᄆ': '마', 'ᄇ': '바', 'ᄈ': '빠', 'ᄉ': '사', 'ᄊ': '싸', 'ᄋ': '아',\n",
    "                'ᄌ': '자', 'ᄍ': '짜', 'ᄎ': '차', 'ᄏ': '카', 'ᄐ': '타', 'ᄑ': '파', 'ᄒ': '하'},\n",
    "    \"second\": {\"ᅡ\":\"아\",\"ᅢ\":\"애\",\"ᅣ\":\"야\",\"ᅤ\":\"얘\",\"ᅥ\":\"어\",\"ᅦ\":\"에\",\"ᅧ\":\"여\",\n",
    "                \"ᅨ\":\"예\",\"ᅩ\":\"오\",\"ᅪ\":\"와\",\"ᅫ\":\"왜\",\"ᅬ\":\"외\",\"ᅭ\":\"요\",\"ᅮ\":\"우\",\n",
    "                \"ᅯ\":\"워\",\"ᅰ\":\"웨\",\"ᅱ\":\"위\",\"ᅲ\":\"유\",\"ᅳ\":\"으\",\"ᅴ\":\"의\",\"ᅵ\":\"이\"},\n",
    "    \"third\" : {\"ᆨ\":\"윽\",\"ᆫ\":\"은\",\"ᆮ\":\"읃\",\"ᆯ\":\"을\",\"ᆷ\":\"음\",\"ᆸ\":\"읍\",\"ᆺ\":\"읏\",\n",
    "                \"ᆼ\":\"응\",\"ᆽ\":\"읒\",\"ᆾ\":\"읓\",\"ᆿ\":\"읔\",\"ᇀ\":\"읕\",\"ᇁ\":\"읖\",\"ᇂ\":\"읗\",\n",
    "                \"ᆩ\":\"윾\",\"ᆻ\":\"읐\",\"ᆪ\":\"윿\",\"ᆬ\":\"읁\",\"ᆭ\":\"읂\",\"ᆰ\":\"읅\",\"ᆱ\":\"읆\",\n",
    "                \"ᆲ\":\"읇\",\"ᆳ\":\"읈\",\"ᆴ\":\"읉\",\"ᆵ\":\"읊\",\"ᆶ\":\"읋\",\"ᆹ\":\"읎\"}   \n",
    "    }\n",
    "initial_list = [\n",
    "    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n",
    "    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n",
    "]\n",
    "# 중성 리스트\n",
    "medial_list = [\n",
    "    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', \n",
    "    'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ'\n",
    "]\n",
    "# 종성 리스트\n",
    "final_list = [\n",
    "    '', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', \n",
    "    'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', \n",
    "    'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n",
    "]\n",
    "\n",
    "\n",
    "class_to_grapheme = {k: {_v:_k for _k, _v in v.items()} for k, v in grapheme_to_class.items()}\n",
    "\n",
    "def f(pred):\n",
    "    for grapheme in [\"first\", \"second\", \"third\"]:\n",
    "        pred[grapheme] = [class_to_grapheme[grapheme].get(x, x) for x in pred[grapheme]]\n",
    "    return list(zip(*pred.values()))\n",
    "    return [compose_korean_char(x, y, z) for x, y, z in zip(*pred.values())]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "x = {'first': '하마하 아아ㄲㅆㅉ', 'second': '아여우 아아ㄲㅆㅉ', 'third': '은응은 읂으ㄲㅆㅉ'}\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': {'가': 'ᄀ',\n",
       "  '까': 'ᄁ',\n",
       "  '나': 'ᄂ',\n",
       "  '다': 'ᄃ',\n",
       "  '따': 'ᄄ',\n",
       "  '라': 'ᄅ',\n",
       "  '마': 'ᄆ',\n",
       "  '바': 'ᄇ',\n",
       "  '빠': 'ᄈ',\n",
       "  '사': 'ᄉ',\n",
       "  '싸': 'ᄊ',\n",
       "  '아': 'ᄋ',\n",
       "  '자': 'ᄌ',\n",
       "  '짜': 'ᄍ',\n",
       "  '차': 'ᄎ',\n",
       "  '카': 'ᄏ',\n",
       "  '타': 'ᄐ',\n",
       "  '파': 'ᄑ',\n",
       "  '하': 'ᄒ'},\n",
       " 'second': {'아': 'ᅡ',\n",
       "  '애': 'ᅢ',\n",
       "  '야': 'ᅣ',\n",
       "  '얘': 'ᅤ',\n",
       "  '어': 'ᅥ',\n",
       "  '에': 'ᅦ',\n",
       "  '여': 'ᅧ',\n",
       "  '예': 'ᅨ',\n",
       "  '오': 'ᅩ',\n",
       "  '와': 'ᅪ',\n",
       "  '왜': 'ᅫ',\n",
       "  '외': 'ᅬ',\n",
       "  '요': 'ᅭ',\n",
       "  '우': 'ᅮ',\n",
       "  '워': 'ᅯ',\n",
       "  '웨': 'ᅰ',\n",
       "  '위': 'ᅱ',\n",
       "  '유': 'ᅲ',\n",
       "  '으': 'ᅳ',\n",
       "  '의': 'ᅴ',\n",
       "  '이': 'ᅵ'},\n",
       " 'third': {'윽': 'ᆨ',\n",
       "  '은': 'ᆫ',\n",
       "  '읃': 'ᆮ',\n",
       "  '을': 'ᆯ',\n",
       "  '음': 'ᆷ',\n",
       "  '읍': 'ᆸ',\n",
       "  '읏': 'ᆺ',\n",
       "  '응': 'ᆼ',\n",
       "  '읒': 'ᆽ',\n",
       "  '읓': 'ᆾ',\n",
       "  '읔': 'ᆿ',\n",
       "  '읕': 'ᇀ',\n",
       "  '읖': 'ᇁ',\n",
       "  '읗': 'ᇂ',\n",
       "  '윾': 'ᆩ',\n",
       "  '읐': 'ᆻ',\n",
       "  '윿': 'ᆪ',\n",
       "  '읁': 'ᆬ',\n",
       "  '읂': 'ᆭ',\n",
       "  '읅': 'ᆰ',\n",
       "  '읆': 'ᆱ',\n",
       "  '읇': 'ᆲ',\n",
       "  '읈': 'ᆳ',\n",
       "  '읉': 'ᆴ',\n",
       "  '읊': 'ᆵ',\n",
       "  '읋': 'ᆶ',\n",
       "  '읎': 'ᆹ'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
