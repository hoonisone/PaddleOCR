{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/home/code/PaddleOCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 682525/682525 [00:10<00:00, 67578.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ppocr.utils.korean_compose import decompose_korean_char\n",
    "import tqdm\n",
    "\n",
    "path = Path(\"/home/datasets/aihub_rec/horizontal_label.txt\")\n",
    "target_path = Path(\"/home/datasets/aihub_rec/horizontal_label2.txt\")\n",
    "target_path.unlink()\n",
    "with open(path) as f:\n",
    "    lines = [line.rstrip(\"\\n\").split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "with open(target_path, \"a+\") as f:\n",
    "    for path, label in tqdm.tqdm(lines):\n",
    "        decomposed = decompose_korean_char(label)\n",
    "        first = \"\".join([x[0] for x in decomposed])\n",
    "        second = \"\".join([x[1] for x in decomposed])\n",
    "        third = \"\".join([x[2] for x in decomposed])\n",
    "        dict_label = {\n",
    "            \"label\":label,\n",
    "            \"first\":first,\n",
    "            \"second\":second,\n",
    "            \"third\":third,\n",
    "        }\n",
    "        f.write(f\"\"\"{path}\\t{str(dict_label).replace(\"'\", '\"')}\\n\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안', '그', '나'), ('녕', '래', '도')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"안녕\"\n",
    "b = \"그래\"\n",
    "c = \"나도\"\n",
    "list(zip(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': '하마하 아아ㄲㅆㅉ', 'second': '아여우 아아ㄲㅆㅉ', 'third': '은응은 읂으ㄲㅆㅉ'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "first_grapheme_dict = {\n",
    "            'ᄀ': '가', 'ᄁ': '까', 'ᄂ': '나', 'ᄃ': '다', 'ᄄ': '따', 'ᄅ': '라',\n",
    "            'ᄆ': '마', 'ᄇ': '바', 'ᄈ': '빠', 'ᄉ': '사', 'ᄊ': '싸', 'ᄋ': '아',\n",
    "            'ᄌ': '자', 'ᄍ': '짜', 'ᄎ': '차', 'ᄏ': '카', 'ᄐ': '타', 'ᄑ': '파', 'ᄒ': '하'\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_first_grapheme(text, type=\"first\"):\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 초성 구하기\n",
    "            choseong_index = (ord(char) - 44032) // 588\n",
    "            # 초성을 문자로 변환하여 결과에 추가\n",
    "            choseong = chr(choseong_index + 0x1100)  # 초성 유니코드 시작값은 0x1100입니다.\n",
    "            result += first_grapheme_dict[choseong]\n",
    "        else: # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "            \n",
    "    return result\n",
    "\n",
    "sceond_grapheme_dict = {\"ᅡ\":\"아\",\"ᅢ\":\"애\",\"ᅣ\":\"야\",\"ᅤ\":\"얘\",\"ᅥ\":\"어\",\"ᅦ\":\"에\",\"ᅧ\":\"여\",\n",
    "\"ᅨ\":\"예\",\"ᅩ\":\"오\",\"ᅪ\":\"와\",\"ᅫ\":\"왜\",\"ᅬ\":\"외\",\"ᅭ\":\"요\",\"ᅮ\":\"우\",\n",
    "\"ᅯ\":\"워\",\"ᅰ\":\"웨\",\"ᅱ\":\"위\",\"ᅲ\":\"유\",\"ᅳ\":\"으\",\"ᅴ\":\"의\",\"ᅵ\":\"이\"}\n",
    "\n",
    "\n",
    "def extract_second_grapheme(text):\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 중성 구하기\n",
    "            jungseong_index = ((ord(char) - 44032) % 588) // 28\n",
    "            # 중성을 문자로 변환하여 결과에 추가\n",
    "            jungseong = chr(jungseong_index + 0x1161)  # 중성 유니코드 시작값은 0x1161입니다.\n",
    "            result += sceond_grapheme_dict[jungseong]\n",
    "        else:\n",
    "            # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "third_grapheme_dict = {\"ᆨ\":\"윽\",\"ᆫ\":\"은\",\"ᆮ\":\"읃\",\"ᆯ\":\"을\",\"ᆷ\":\"음\",\"ᆸ\":\"읍\",\"ᆺ\":\"읏\",\n",
    "\"ᆼ\":\"응\",\"ᆽ\":\"읒\",\"ᆾ\":\"읓\",\"ᆿ\":\"읔\",\"ᇀ\":\"읕\",\"ᇁ\":\"읖\",\"ᇂ\":\"읗\",\n",
    "\"ᆩ\":\"윾\",\"ᆻ\":\"읐\",\"ᆪ\":\"윿\",\"ᆬ\":\"읁\",\"ᆭ\":\"읂\",\"ᆰ\":\"읅\",\"ᆱ\":\"읆\",\n",
    "\"ᆲ\":\"읇\",\"ᆳ\":\"읈\",\"ᆴ\":\"읉\",\"ᆵ\":\"읊\",\"ᆶ\":\"읋\",\"ᆹ\":\"읎\"}\n",
    "\n",
    "\n",
    "def extract_third_grapheme(text):\n",
    "    result = ''\n",
    "    has_jongseong = False  # 받침이 있는지 여부를 판별하기 위한 변수\n",
    "\n",
    "    for char in text:\n",
    "        # 한글 유니코드 범위인지 확인\n",
    "        if 44032 <= ord(char) <= 55203:\n",
    "            # 종성 구하기\n",
    "            jongseong_index = (ord(char) - 44032) % 28\n",
    "\n",
    "            # 받침이 있는 경우에만 결과에 추가하고, 받침이 없는 경우 \"으\"를 추가\n",
    "            if jongseong_index != 0:\n",
    "                jongseong = chr(jongseong_index + 0x11A7)  # 종성 유니코드 시작값은 0x11A8이 아닌 0x11A7입니다.\n",
    "                result += third_grapheme_dict[jongseong]\n",
    "            else:\n",
    "                result += '으'\n",
    "        else:\n",
    "            # 한글이 아니면 그대로 결과에 추가\n",
    "            result += char\n",
    "    \n",
    "    return result\n",
    "\n",
    "@functools.lru_cache(maxsize=1000000)\n",
    "def extract_grapheme(text):\n",
    "    return {\n",
    "        \"first\":extract_first_grapheme(text),\n",
    "        \"second\":extract_second_grapheme(text),\n",
    "        \"third\":extract_third_grapheme(text)\n",
    "    }\n",
    "\n",
    "extract_grapheme(\"한명훈 않아ㄲㅆㅉ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ᄒ', 'ᅡ', 'ᆫ'),\n",
       " ('ᄆ', 'ᅧ', 'ᆼ'),\n",
       " ('ᄒ', 'ᅮ', 'ᆫ'),\n",
       " (' ', ' ', ' '),\n",
       " ('ᄋ', 'ᅡ', 'ᆭ'),\n",
       " ('ᄋ', 'ᅡ', '으'),\n",
       " ('ㄲ', 'ㄲ', 'ㄲ'),\n",
       " ('ㅆ', 'ㅆ', 'ㅆ'),\n",
       " ('ㅉ', 'ㅉ', 'ㅉ')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grapheme_to_class = {\n",
    "    \"first\" : {'ᄀ': '가', 'ᄁ': '까', 'ᄂ': '나', 'ᄃ': '다', 'ᄄ': '따', 'ᄅ': '라',\n",
    "                'ᄆ': '마', 'ᄇ': '바', 'ᄈ': '빠', 'ᄉ': '사', 'ᄊ': '싸', 'ᄋ': '아',\n",
    "                'ᄌ': '자', 'ᄍ': '짜', 'ᄎ': '차', 'ᄏ': '카', 'ᄐ': '타', 'ᄑ': '파', 'ᄒ': '하'},\n",
    "    \"second\": {\"ᅡ\":\"아\",\"ᅢ\":\"애\",\"ᅣ\":\"야\",\"ᅤ\":\"얘\",\"ᅥ\":\"어\",\"ᅦ\":\"에\",\"ᅧ\":\"여\",\n",
    "                \"ᅨ\":\"예\",\"ᅩ\":\"오\",\"ᅪ\":\"와\",\"ᅫ\":\"왜\",\"ᅬ\":\"외\",\"ᅭ\":\"요\",\"ᅮ\":\"우\",\n",
    "                \"ᅯ\":\"워\",\"ᅰ\":\"웨\",\"ᅱ\":\"위\",\"ᅲ\":\"유\",\"ᅳ\":\"으\",\"ᅴ\":\"의\",\"ᅵ\":\"이\"},\n",
    "    \"third\" : {\"ᆨ\":\"윽\",\"ᆫ\":\"은\",\"ᆮ\":\"읃\",\"ᆯ\":\"을\",\"ᆷ\":\"음\",\"ᆸ\":\"읍\",\"ᆺ\":\"읏\",\n",
    "                \"ᆼ\":\"응\",\"ᆽ\":\"읒\",\"ᆾ\":\"읓\",\"ᆿ\":\"읔\",\"ᇀ\":\"읕\",\"ᇁ\":\"읖\",\"ᇂ\":\"읗\",\n",
    "                \"ᆩ\":\"윾\",\"ᆻ\":\"읐\",\"ᆪ\":\"윿\",\"ᆬ\":\"읁\",\"ᆭ\":\"읂\",\"ᆰ\":\"읅\",\"ᆱ\":\"읆\",\n",
    "                \"ᆲ\":\"읇\",\"ᆳ\":\"읈\",\"ᆴ\":\"읉\",\"ᆵ\":\"읊\",\"ᆶ\":\"읋\",\"ᆹ\":\"읎\"}   \n",
    "    }\n",
    "initial_list = [\n",
    "    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n",
    "    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n",
    "]\n",
    "# 중성 리스트\n",
    "medial_list = [\n",
    "    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', \n",
    "    'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ'\n",
    "]\n",
    "# 종성 리스트\n",
    "final_list = [\n",
    "    '', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', \n",
    "    'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', \n",
    "    'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n",
    "]\n",
    "\n",
    "\n",
    "class_to_grapheme = {k: {_v:_k for _k, _v in v.items()} for k, v in grapheme_to_class.items()}\n",
    "\n",
    "def f(pred):\n",
    "    for grapheme in [\"first\", \"second\", \"third\"]:\n",
    "        pred[grapheme] = [class_to_grapheme[grapheme].get(x, x) for x in pred[grapheme]]\n",
    "    return list(zip(*pred.values()))\n",
    "    return [compose_korean_char(x, y, z) for x, y, z in zip(*pred.values())]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "x = {'first': '하마하 아아ㄲㅆㅉ', 'second': '아여우 아아ㄲㅆㅉ', 'third': '은응은 읂으ㄲㅆㅉ'}\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': {'가': 'ᄀ',\n",
       "  '까': 'ᄁ',\n",
       "  '나': 'ᄂ',\n",
       "  '다': 'ᄃ',\n",
       "  '따': 'ᄄ',\n",
       "  '라': 'ᄅ',\n",
       "  '마': 'ᄆ',\n",
       "  '바': 'ᄇ',\n",
       "  '빠': 'ᄈ',\n",
       "  '사': 'ᄉ',\n",
       "  '싸': 'ᄊ',\n",
       "  '아': 'ᄋ',\n",
       "  '자': 'ᄌ',\n",
       "  '짜': 'ᄍ',\n",
       "  '차': 'ᄎ',\n",
       "  '카': 'ᄏ',\n",
       "  '타': 'ᄐ',\n",
       "  '파': 'ᄑ',\n",
       "  '하': 'ᄒ'},\n",
       " 'second': {'아': 'ᅡ',\n",
       "  '애': 'ᅢ',\n",
       "  '야': 'ᅣ',\n",
       "  '얘': 'ᅤ',\n",
       "  '어': 'ᅥ',\n",
       "  '에': 'ᅦ',\n",
       "  '여': 'ᅧ',\n",
       "  '예': 'ᅨ',\n",
       "  '오': 'ᅩ',\n",
       "  '와': 'ᅪ',\n",
       "  '왜': 'ᅫ',\n",
       "  '외': 'ᅬ',\n",
       "  '요': 'ᅭ',\n",
       "  '우': 'ᅮ',\n",
       "  '워': 'ᅯ',\n",
       "  '웨': 'ᅰ',\n",
       "  '위': 'ᅱ',\n",
       "  '유': 'ᅲ',\n",
       "  '으': 'ᅳ',\n",
       "  '의': 'ᅴ',\n",
       "  '이': 'ᅵ'},\n",
       " 'third': {'윽': 'ᆨ',\n",
       "  '은': 'ᆫ',\n",
       "  '읃': 'ᆮ',\n",
       "  '을': 'ᆯ',\n",
       "  '음': 'ᆷ',\n",
       "  '읍': 'ᆸ',\n",
       "  '읏': 'ᆺ',\n",
       "  '응': 'ᆼ',\n",
       "  '읒': 'ᆽ',\n",
       "  '읓': 'ᆾ',\n",
       "  '읔': 'ᆿ',\n",
       "  '읕': 'ᇀ',\n",
       "  '읖': 'ᇁ',\n",
       "  '읗': 'ᇂ',\n",
       "  '윾': 'ᆩ',\n",
       "  '읐': 'ᆻ',\n",
       "  '윿': 'ᆪ',\n",
       "  '읁': 'ᆬ',\n",
       "  '읂': 'ᆭ',\n",
       "  '읅': 'ᆰ',\n",
       "  '읆': 'ᆱ',\n",
       "  '읇': 'ᆲ',\n",
       "  '읈': 'ᆳ',\n",
       "  '읉': 'ᆴ',\n",
       "  '읊': 'ᆵ',\n",
       "  '읋': 'ᆶ',\n",
       "  '읎': 'ᆹ'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅇㅏ@ㄹㅗ@ㅁㅏ@\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아로마'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HANGUL_BASE = 0xAC00\n",
    "CHO_BASE = 0x1100\n",
    "JUNG_BASE = 0x1161\n",
    "JONG_BASE = 0x11A7\n",
    "\n",
    "# 초성, 중성, 종성 테이블\n",
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "JONGSUNG_LIST = ['@', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "def decompose_hangul_by_utf8(text):\n",
    "    s = \"\"\n",
    "    for char in text:\n",
    "        if not (HANGUL_BASE <= ord(char) <= HANGUL_BASE + 11171):\n",
    "            s += char\n",
    "            continue\n",
    "        \n",
    "        # 한글 문자 코드\n",
    "        char_code = ord(char) - HANGUL_BASE\n",
    "        jong = char_code % 28\n",
    "        jung = ((char_code - jong) // 28) % 21\n",
    "        cho = ((char_code - jong) // 28) // 21\n",
    "        \n",
    "        s +=  CHOSUNG_LIST[cho]+JUNGSUNG_LIST[jung]+JONGSUNG_LIST[jong]\n",
    "    return s\n",
    "\n",
    "\n",
    "def compose_hangul_by_utf8(cho, jung, jong=''):\n",
    "    # 초성, 중성, 종성을 결합하여 유니코드 한글 문자로 변환\n",
    "    cho_index = CHOSUNG_LIST.index(cho)\n",
    "    jung_index = JUNGSUNG_LIST.index(jung)\n",
    "    jong_index = JONGSUNG_LIST.index(jong) if jong else 0\n",
    "    \n",
    "    char_code = HANGUL_BASE + (cho_index * 21 + jung_index) * 28 + jong_index\n",
    "    return chr(char_code)\n",
    "\n",
    "def compose_string_by_utf8(decomposed_list):\n",
    "    composed = []\n",
    "    i = 0\n",
    "    while i < len(decomposed_list):\n",
    "        # 초성, 중성, 종성이 이어진 한글 문자인지 확인\n",
    "        if decomposed_list[i] in CHOSUNG_LIST:\n",
    "            if i + 1 < len(decomposed_list) and decomposed_list[i+1] in JUNGSUNG_LIST:\n",
    "                cho = decomposed_list[i]\n",
    "                jung = decomposed_list[i+1]\n",
    "                jong = decomposed_list[i+2] if i + 2 < len(decomposed_list) and decomposed_list[i+2] in JONGSUNG_LIST else ''\n",
    "                composed.append(compose_hangul_by_utf8(cho, jung, jong))\n",
    "                i += 3 if jong else 2  # 종성이 있으면 3칸, 없으면 2칸 이동\n",
    "            else:\n",
    "                composed.append(decomposed_list[i])  # 중성이 없으면 그대로 추가\n",
    "                i += 1\n",
    "        else:\n",
    "            composed.append(decomposed_list[i])  # 한글이 아니면 그대로 추가\n",
    "            i += 1\n",
    "    return ''.join(composed)\n",
    "\n",
    "# 예시: '한'\n",
    "decomposed = decompose_hangul_by_utf8('아로마')  # 출력: ('ㅎ', 'ㅏ', 'ㄴ')\n",
    "print(decomposed)\n",
    "compose_string_by_utf8(decomposed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅣㄼ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compose_string_by_utf8(\"ㅣㄼ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "CONSONANTS = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㄳ', 'ㄵ', 'ㄶ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅄ']\n",
    "VOWELS = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "CONSONANTS = {'ㄱ': '가',\n",
    " 'ㄲ': '끄',\n",
    " 'ㄴ': '느',\n",
    " 'ㄷ': '드',\n",
    " 'ㄸ': '뜨',\n",
    " 'ㄹ': '르',\n",
    " 'ㅁ': '므',\n",
    " 'ㅂ': '브',\n",
    " 'ㅃ': '쁘',\n",
    " 'ㅅ': '스',\n",
    " 'ㅆ': '쓰',\n",
    " 'ㅇ': '으',\n",
    " 'ㅈ': '즈',\n",
    " 'ㅉ': '쯔',\n",
    " 'ㅊ': '츠',\n",
    " 'ㅋ': '크',\n",
    " 'ㅌ': '트',\n",
    " 'ㅍ': '프',\n",
    " 'ㅎ': '프',\n",
    " 'ㄳ': '윿',\n",
    " 'ㄵ': '읁',\n",
    " 'ㄶ': '읂',\n",
    " 'ㄺ': '읅',\n",
    " 'ㄻ': '읆',\n",
    " 'ㄼ': '읇',\n",
    " 'ㄽ': '읈',\n",
    " 'ㄾ': '읉',\n",
    " 'ㄿ': '읊',\n",
    " 'ㅀ': '읋',\n",
    " 'ㅄ': '읎',\n",
    " '으': ''}\n",
    "\n",
    "VOWELS = {'ㅏ': '아', \n",
    "'ㅐ': '애', \n",
    "'ㅑ': '야', \n",
    "'ㅒ': '얘', \n",
    "'ㅓ': '어', \n",
    "'ㅔ': '에', \n",
    "'ㅕ': '여', \n",
    "'ㅖ': '예', \n",
    "'ㅗ': '오', \n",
    "'ㅘ': '와', \n",
    "'ㅙ': '왜', \n",
    "'ㅚ': '외', \n",
    "'ㅛ': '요', \n",
    "'ㅜ': '우', \n",
    "'ㅝ': '워', \n",
    "'ㅞ': '웨', \n",
    "'ㅟ': '위', \n",
    "'ㅠ': '유', \n",
    "'ㅡ': '응', \n",
    "'ㅢ': '의', \n",
    "'ㅣ': '이'}\n",
    "\n",
    "print(len(CONSONANTS))\n",
    "print(len(VOWELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'ㄱ',\n",
       " 'ㄲ',\n",
       " 'ㄳ',\n",
       " 'ㄴ',\n",
       " 'ㄵ',\n",
       " 'ㄶ',\n",
       " 'ㄷ',\n",
       " 'ㄸ',\n",
       " 'ㄹ',\n",
       " 'ㄺ',\n",
       " 'ㄻ',\n",
       " 'ㄼ',\n",
       " 'ㄽ',\n",
       " 'ㄾ',\n",
       " 'ㄿ',\n",
       " 'ㅀ',\n",
       " 'ㅁ',\n",
       " 'ㅂ',\n",
       " 'ㅃ',\n",
       " 'ㅄ',\n",
       " 'ㅅ',\n",
       " 'ㅆ',\n",
       " 'ㅇ',\n",
       " 'ㅈ',\n",
       " 'ㅉ',\n",
       " 'ㅊ',\n",
       " 'ㅋ',\n",
       " 'ㅌ',\n",
       " 'ㅍ',\n",
       " 'ㅎ',\n",
       " 'ㅏ',\n",
       " 'ㅐ',\n",
       " 'ㅑ',\n",
       " 'ㅒ',\n",
       " 'ㅓ',\n",
       " 'ㅔ',\n",
       " 'ㅕ',\n",
       " 'ㅖ',\n",
       " 'ㅗ',\n",
       " 'ㅘ',\n",
       " 'ㅙ',\n",
       " 'ㅚ',\n",
       " 'ㅛ',\n",
       " 'ㅜ',\n",
       " 'ㅝ',\n",
       " 'ㅞ',\n",
       " 'ㅟ',\n",
       " 'ㅠ',\n",
       " 'ㅡ',\n",
       " 'ㅢ',\n",
       " 'ㅣ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "JONGSUNG_LIST = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "x = list(set(CHOSUNG_LIST) | set(JUNGSUNG_LIST) | set(JONGSUNG_LIST))\n",
    "x.sort()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅎ', 'ㅏ', 'ㄺ', 's', 'u', 'ㅎ', 'ㅜ', 'ㄳ', 'ㅎ', 'ㅏ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decompose_string_utf8(input_str):\n",
    "    decomposed = []\n",
    "    for char in input_str:\n",
    "        if HANGUL_BASE <= ord(char) <= HANGUL_BASE + 11171:\n",
    "            decomposed.extend(decompose_hangul(char))  # 한글이면 자소로 분해\n",
    "        else:\n",
    "            decomposed.append(char)  # 한글이 아니면 그대로 추가\n",
    "    return decomposed\n",
    "\n",
    "decompose_string_utf8(\"핡su훇하\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
