{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {1:1}\n",
    "x.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean_abi_rec_v1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300]\n",
      "rec_grapheme_10k_horizontal_C [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_10k_horizontal_C+F [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_10k_horizontal_C+FM [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_10k_horizontal_C+FML [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_10k_horizontal_G [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_20k_horizontal_C [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_20k_horizontal_C+F [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_20k_horizontal_C+FM [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_20k_horizontal_C+FML [180, 190, 195, 200, 205, 210, 220, 230, 240, 245, 250, 260, 270, 280, 285, 290, 300]\n",
      "rec_grapheme_20k_horizontal_G [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_30k_horizontal_C [5, 10, 15, 20, 25, 35, 40, 45, 50, 55, 60, 65, 75, 80, 85, 90, 95, 100, 105, 115, 120, 125, 130, 135, 140, 145, 155, 160, 165, 170, 175, 180, 185, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_30k_horizontal_C+F [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275]\n",
      "rec_grapheme_30k_horizontal_C+FM [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_30k_horizontal_C+FML [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_30k_horizontal_G [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_40k_horizontal_C [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_40k_horizontal_C+F [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255]\n",
      "rec_grapheme_40k_horizontal_C+FM [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210]\n",
      "rec_grapheme_40k_horizontal_C+FML [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_40k_horizontal_G [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_50k_horizontal_C [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_50k_horizontal_C+F [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200]\n",
      "rec_grapheme_50k_horizontal_C+FM [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105]\n",
      "rec_grapheme_50k_horizontal_C+FML [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_50k_horizontal_G [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n",
      "rec_grapheme_full_horizontal_C+FML_v1.0 [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\n",
      "rec_grapheme_full_horizontal_C+FML_v1.1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "rec_grapheme_full_horizontal_C+FML_v1.2 []\n",
      "rec_grapheme_full_horizontal_C+FML_v1.3 [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "rec_grapheme_full_horizontal_C+FML_v1.4 []\n",
      "rec_grapheme_full_horizontal_C+FML_v1.5 [1, 2, 3, 4, 5, 6]\n",
      "rec_grapheme_full_horizontal_C+FM_v1.0 []\n",
      "rec_grapheme_full_horizontal_C+F_v1.0 []\n",
      "rec_grapheme_full_horizontal_C_v1.0 []\n",
      "rec_grapheme_full_horizontal_G_v1.0 [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n"
     ]
    }
   ],
   "source": [
    "from database import *\n",
    "from pathlib import Path\n",
    "workdb = WorkDB()\n",
    "\n",
    "id_list = workdb.get_all_id()\n",
    "id_list = sorted(id_list)\n",
    "for id in id_list:\n",
    "    a = workdb.get_all_epoch(id)\n",
    "    print(id, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character': '지정', 'first': '자자', 'second': '이어', 'third': '으응'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(\"\"\"{'character': '지정', 'first': '자자', 'second': '이어', 'third': '으응'}\"\"\".replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Sample 1:\n",
      "  images: (64, 64, 3)\n",
      "  labels: 0\n",
      "  metadata: {'date': array(['2024-06-19'], dtype='<U10'), 'info': array(['sample1'], dtype='<U7')}\n",
      "\n",
      "Loaded Sample 2:\n",
      "  images: (64, 64, 3)\n",
      "  labels: 1\n",
      "  metadata: {'a': {'a': 1}, 'date': array(['2024-06-20'], dtype='<U10'), 'info': array(['sample2'], dtype='<U7')}\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Path(\"/home/dataset_cache.h5\").unlink()\n",
    "cache_file = \"/home/dataset_cache.h5\"\n",
    "# HDF5 파일에 샘플 데이터 저장\n",
    "def save_dict_to_hdf5(group, data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, np.ndarray) and value.dtype.type is np.str_:\n",
    "                dt = h5py.special_dtype(vlen=str)\n",
    "                group.create_dataset(key, data=value.astype(dt))\n",
    "            else:\n",
    "                group.create_dataset(key, data=value)\n",
    "\n",
    "def save_samples_to_hdf5(data_sample, sample_key, filename = cache_file):\n",
    "    with h5py.File(filename, 'a') as f:\n",
    "        if sample_key in f:\n",
    "            return\n",
    "        group = f.create_group(sample_key)\n",
    "        save_dict_to_hdf5(group, data_sample)\n",
    "\n",
    "# HDF5 파일에서 샘플 데이터 로드\n",
    "def load_dict_from_hdf5(group):\n",
    "    data = {}\n",
    "    for key in group.keys():\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            data[key] = load_dict_from_hdf5(group[key])\n",
    "        else:\n",
    "            dataset = group[key]\n",
    "            if dataset.shape == ():  # 스칼라 데이터셋\n",
    "                value = dataset[()]\n",
    "            else:\n",
    "                value = dataset[:]\n",
    "                if isinstance(value, np.ndarray) and value.dtype.type is np.bytes_:\n",
    "                    value = value.astype(str)\n",
    "            data[key] = value\n",
    "    return data\n",
    "\n",
    "def load_samples_from_hdf5(sample_key, filename = cache_file):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        sample = load_dict_from_hdf5(f[sample_key])\n",
    "    return sample\n",
    "\n",
    "# 사용 예\n",
    "data_sample1 = {\n",
    "    'images': np.random.rand(64, 64, 3),\n",
    "    'labels': np.array(0),\n",
    "    'metadata': {\n",
    "        'info': np.array(['sample1'], dtype='S'),\n",
    "        'date': np.array(['2024-06-19'], dtype='S')\n",
    "    }\n",
    "}\n",
    "\n",
    "data_sample2 = {\n",
    "    'images': np.random.rand(64, 64, 3),\n",
    "    'labels': np.array(1),\n",
    "    'metadata': {\n",
    "        'info': np.array(['sample2'], dtype='S'),\n",
    "        'date': np.array(['2024-06-20'], dtype='S'),\n",
    "        \"a\":{\"a\":1}\n",
    "    }\n",
    "}\n",
    "\n",
    "# 샘플 데이터 저장\n",
    "save_samples_to_hdf5(data_sample1, 'key_3')\n",
    "save_samples_to_hdf5(data_sample2, 'key_4')\n",
    "\n",
    "# 샘플 데이터 로드\n",
    "loaded_sample1 = load_samples_from_hdf5('key_3')\n",
    "loaded_sample2 = load_samples_from_hdf5('key_4')\n",
    "\n",
    "# 로드된 데이터 출력\n",
    "print(\"Loaded Sample 1:\")\n",
    "for key, value in loaded_sample1.items():\n",
    "    print(f\"  {key}: {value.shape if isinstance(value, np.ndarray) else value}\")\n",
    "\n",
    "print(\"\\nLoaded Sample 2:\")\n",
    "for key, value in loaded_sample2.items():\n",
    "    print(f\"  {key}: {value.shape if isinstance(value, np.ndarray) else value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Sample 1:\n",
      "  images: (64, 64, 3)\n",
      "  labels: 0\n",
      "  metadata: {'date': array(['2024-06-19'], dtype='<U10'), 'info': array(['sample1'], dtype='<U7')}\n",
      "\n",
      "Loaded Sample 2:\n",
      "  images: (64, 64, 3)\n",
      "  labels: 1\n",
      "  metadata: {'a': {'a': 1}, 'date': array(['2024-06-20'], dtype='<U10'), 'info': array(['sample2'], dtype='<U7')}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/home/code/PaddleOCR/ppocr\")\n",
    "\n",
    "\n",
    "from utils.dataset_cache import save_samples_to_hdf5, load_samples_from_hdf5\n",
    "\n",
    "\n",
    "\n",
    "data_sample1 = {\n",
    "    'images': np.random.rand(64, 64, 3),\n",
    "    'labels': np.array(0),\n",
    "    'metadata': {\n",
    "        'info': np.array(['sample1'], dtype='S'),\n",
    "        'date': np.array(['2024-06-19'], dtype='S')\n",
    "    }\n",
    "}\n",
    "\n",
    "data_sample2 = {\n",
    "    'images': np.random.rand(64, 64, 3),\n",
    "    'labels': np.array(1),\n",
    "    'metadata': {\n",
    "        'info': np.array(['sample2'], dtype='S'),\n",
    "        'date': np.array(['2024-06-20'], dtype='S'),\n",
    "        \"a\":{\"a\":1}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# 샘플 데이터 저장\n",
    "save_samples_to_hdf5(data_sample1, 'key_1')\n",
    "save_samples_to_hdf5(data_sample2, 'key_2')\n",
    "\n",
    "# 샘플 데이터 로드\n",
    "loaded_sample1 = load_samples_from_hdf5('key_1')\n",
    "loaded_sample2 = load_samples_from_hdf5('key_2')\n",
    "\n",
    "# 로드된 데이터 출력\n",
    "print(\"Loaded Sample 1:\")\n",
    "for key, value in loaded_sample1.items():\n",
    "    print(f\"  {key}: {value.shape if isinstance(value, np.ndarray) else value}\")\n",
    "\n",
    "print(\"\\nLoaded Sample 2:\")\n",
    "for key, value in loaded_sample2.items():\n",
    "    print(f\"  {key}: {value.shape if isinstance(value, np.ndarray) else value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421805"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import e\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from filelock import FileLock\n",
    "\n",
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "\n",
    "# Path(\"/home/dataset_cache.h5\").unlink()\n",
    "cache_file = \"/home/dataset_cache.h5\"\n",
    "lock = FileLock(f\"{cache_file}.lock\", timeout=10)\n",
    "\n",
    "# HDF5 파일에 샘플 데이터 저장\n",
    "def save_dict_to_hdf5(group, data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, np.ndarray) and value.dtype.type is np.str_:\n",
    "                dt = h5py.special_dtype(vlen=str)\n",
    "                group.create_dataset(key, data=value.astype(dt))\n",
    "            else:\n",
    "                group.create_dataset(key, data=value)\n",
    "\n",
    "def save_samples_to_hdf5(data_sample, sample_key, filename = cache_file):\n",
    "    while True:\n",
    "        try:\n",
    "            with lock:\n",
    "                with h5py.File(filename, 'a') as f:\n",
    "                    if sample_key in f:\n",
    "                        return\n",
    "                    group = f.create_group(sample_key)\n",
    "                    save_dict_to_hdf5(group, data_sample)\n",
    "                    f.flush()\n",
    "                    fd = f.id.get_vfd_handle()  # 파일 기술자 가져오기\n",
    "                    os.fsync(fd)  # 운영 체제 파일 시스템 버퍼 플러시\n",
    "                break\n",
    "        except BlockingIOError as e:\n",
    "            continue\n",
    "\n",
    "# # HDF5 파일에서 샘플 데이터 로드\n",
    "# def load_dict_from_hdf5(group):\n",
    "#     data = {}\n",
    "#     for key in group.keys():\n",
    "#         if isinstance(group[key], h5py.Group):\n",
    "#             data[key] = load_dict_from_hdf5(group[key])\n",
    "#         else:\n",
    "#             dataset = group[key]\n",
    "#             if dataset.shape == ():  # 스칼라 데이터셋\n",
    "#                 value = dataset[()]\n",
    "#             else:\n",
    "#                 value = dataset[:]\n",
    "#                 if isinstance(value, np.ndarray) and value.dtype.type is np.bytes_:\n",
    "#                     value = value.astype(str)\n",
    "#             data[key] = value\n",
    "#     return data\n",
    "\n",
    "def load_dict_from_hdf5(group):\n",
    "    data = {}\n",
    "    for key in group.keys():\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            data[key] = load_dict_from_hdf5(group[key])\n",
    "        else:\n",
    "            dataset = group[key]\n",
    "            if dataset.shape == ():  # 스칼라 데이터셋\n",
    "                value = dataset[()]\n",
    "                if isinstance(value, (bytes, np.bytes_)):\n",
    "                    value = value.decode('utf-8')  # 바이트를 문자열로 디코딩\n",
    "            else:\n",
    "                value = dataset[:]\n",
    "                if isinstance(value, np.ndarray) and value.dtype.type is np.bytes_:\n",
    "                    value = value.astype(str)\n",
    "                elif isinstance(value, np.ndarray) and value.dtype.type is np.object_:\n",
    "                    value = np.array([item.decode('utf-8') if isinstance(item, (bytes, np.bytes_)) else item for item in value])\n",
    "            data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_samples_from_hdf5(sample_key, filename = cache_file):\n",
    "    if not Path(filename).exists():\n",
    "        return None\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if sample_key in f:\n",
    "            return load_dict_from_hdf5(f[sample_key])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "a = 0\n",
    "with lock:\n",
    "    with h5py.File(cache_file, 'r') as f:\n",
    "        for key in f[\"datasets\"][\"aihub_rec\"].keys():\n",
    "            try:\n",
    "                a  += len(f[\"datasets\"][\"aihub_rec\"][key].keys())\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            \n",
    "a\n",
    "# load_samples_from_hdf5(\"./datasets/aihub_rec/718/717116.png\").keys()\n",
    "# load_samples_from_hdf5(\"./datasets/aihub_rec/718/717116.png\")[\"origin_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: FileLock in /home/.venv/lib/python3.7/site-packages (3.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install FileLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_list(data, num_chunks):\n",
    "    avg = len(data) / float(num_chunks)\n",
    "    chunks = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(data):\n",
    "        chunks.append(data[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunk_list(list(range(100)), 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636448"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_setting = {\n",
    "    'rdcc_nbytes': 1024**3*20,  # 1GB\n",
    "    'rdcc_nslots': int(1e6),  # 1백만 슬롯\n",
    "    'rdcc_w0': 0.5  # 조기 쓰기 비율\n",
    "}\n",
    "\n",
    "from math import e\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from filelock import FileLock\n",
    "\n",
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "\n",
    "# Path(\"/home/dataset_cache.h5\").unlink()\n",
    "cache_file = \"/home/dataset_cache.h5\"\n",
    "lock = FileLock(f\"{cache_file}.lock\", timeout=10)\n",
    "\n",
    "# HDF5 파일에 샘플 데이터 저장\n",
    "def save_dict_to_hdf5(group, data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, np.ndarray) and value.dtype.type is np.str_:\n",
    "                dt = h5py.special_dtype(vlen=str)\n",
    "                group.create_dataset(key, data=value.astype(dt))\n",
    "            else:\n",
    "                group.create_dataset(key, data=value)\n",
    "\n",
    "def save_samples_to_hdf5(data_sample, sample_key, filename = cache_file):\n",
    "    while True:\n",
    "        try:\n",
    "            with lock:\n",
    "                with h5py.File(filename, 'w', **cache_setting) as f:\n",
    "                    if sample_key in f:\n",
    "                        return\n",
    "                    group = f.create_group(sample_key)\n",
    "                    save_dict_to_hdf5(group, data_sample)\n",
    "                    f.flush()\n",
    "                    fd = f.id.get_vfd_handle()  # 파일 기술자 가져오기\n",
    "                    os.fsync(fd)  # 운영 체제 파일 시스템 버퍼 플러시\n",
    "                break\n",
    "        except BlockingIOError as e:\n",
    "            continue\n",
    "\n",
    "# # HDF5 파일에서 샘플 데이터 로드\n",
    "# def load_dict_from_hdf5(group):\n",
    "#     data = {}\n",
    "#     for key in group.keys():\n",
    "#         if isinstance(group[key], h5py.Group):\n",
    "#             data[key] = load_dict_from_hdf5(group[key])\n",
    "#         else:\n",
    "#             dataset = group[key]\n",
    "#             if dataset.shape == ():  # 스칼라 데이터셋\n",
    "#                 value = dataset[()]\n",
    "#             else:\n",
    "#                 value = dataset[:]\n",
    "#                 if isinstance(value, np.ndarray) and value.dtype.type is np.bytes_:\n",
    "#                     value = value.astype(str)\n",
    "#             data[key] = value\n",
    "#     return data\n",
    "\n",
    "def load_dict_from_hdf5(group):\n",
    "    data = {}\n",
    "    for key in group.keys():\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            data[key] = load_dict_from_hdf5(group[key])\n",
    "        else:\n",
    "            dataset = group[key]\n",
    "            if dataset.shape == ():  # 스칼라 데이터셋\n",
    "                value = dataset[()]\n",
    "                if isinstance(value, (bytes, np.bytes_)):\n",
    "                    value = value.decode('utf-8')  # 바이트를 문자열로 디코딩\n",
    "            else:\n",
    "                value = dataset[:]\n",
    "                if isinstance(value, np.ndarray) and value.dtype.type is np.bytes_:\n",
    "                    value = value.astype(str)\n",
    "                elif isinstance(value, np.ndarray) and value.dtype.type is np.object_:\n",
    "                    value = np.array([item.decode('utf-8') if isinstance(item, (bytes, np.bytes_)) else item for item in value])\n",
    "            data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_samples_from_hdf5(sample_key, filename = cache_file):\n",
    "    if not Path(filename).exists():\n",
    "        return None\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if sample_key in f:\n",
    "            while True:\n",
    "                try:\n",
    "                    return load_dict_from_hdf5(f[sample_key])\n",
    "                except :\n",
    "                    \n",
    "                    continue\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "a = 0\n",
    "with lock:\n",
    "    with h5py.File(cache_file, 'r') as f:\n",
    "        for key in f[\"datasets\"][\"aihub_rec\"].keys():\n",
    "            try:\n",
    "                a  += len(f[\"datasets\"][\"aihub_rec\"][key].keys())\n",
    "            except BlockingIOError as e:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "a\n",
    "# load_samples_from_hdf5(\"./datasets/aihub_rec/718/717116.png\").keys()\n",
    "# load_samples_from_hdf5(\"./datasets/aihub_rec/718/717116.png\")[\"origin_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py version: 3.8.0\n",
      "HDF5 version: 1.12.2\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# h5py 라이브러리 버전 확인\n",
    "print(\"h5py version:\", h5py.__version__)\n",
    "\n",
    "# 사용 중인 HDF5 라이브러리 버전 확인\n",
    "print(\"HDF5 version:\", h5py.version.hdf5_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {1, 2, 3}\n",
    "y= {3, 4, 5}\n",
    "z = {1, 3, 5}\n",
    "a = [x, y, z]\n",
    "\n",
    "\n",
    "import itertools\n",
    "set(itertools.chain.from_iterable(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdf.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"sdf.pdparams\"\n",
    "a[:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
